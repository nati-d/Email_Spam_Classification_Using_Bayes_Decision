{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading and Sampling the Dataset**\n",
    "\n",
    "In this section, we read the **SMS Spam Collection** dataset into a pandas DataFrame and perform the following tasks:\n",
    "\n",
    "1. **Reading the Dataset**: The dataset is loaded using `pandas.read_csv()` with the specified separator (`'\\t'`) to properly separate the columns.\n",
    "2. **Adding Column Names**: The dataset doesn't have column headers, so we add custom column names: 'Label' (for the label indicating spam or non-spam) and 'Email_Messages' (for the text of the messages).\n",
    "3. **Sampling**: We randomly sample 0.2% of the dataset to work with a smaller subset. This is done to improve performance and reduce memory usage, especially when dealing with large datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the dataset and add columns because the give dataset is not in the proper format by specifying the separator as '\\t' and adding column names as 'Label' and 'Email_Messages'\n",
    "df = pd.read_csv('SMSSpamCollection', sep = '\\t', header = None)\n",
    "df.columns = ['Label', 'Email_Messages']\n",
    "\n",
    "#Sample 0.2 percent of the dataset\n",
    "sample_df = df.sample(frac=0.2, random_state=12)\n",
    "\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating Prior Probabilities for Spam and Non-Spam Messages**\n",
    "\n",
    "In this section, we calculate the **prior probabilities** for spam and non-spam messages based on the data. The prior probability refers to the likelihood of an event (in this case, a message being spam or non-spam) before any further information (features or words) is considered. These probabilities are used as the base or initial assumption for classification in a Naive Bayes model.\n",
    "\n",
    "The **prior probabilities** are calculated as follows:\n",
    "\n",
    "- **Prior probability of spam messages**: This is the proportion of spam messages out of the total number of messages.\n",
    "- **Prior probability of non-spam messages**: This is the proportion of non-spam messages out of the total number of messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of messages in the dataset\n",
    "total_msgs = len(sample_df)\n",
    "\n",
    "# Calculate the number of spam messages\n",
    "# Filter the dataset where the 'Label' column is equal to 'spam'\n",
    "number_of_spam_msgs = len(sample_df[sample_df['Label'] == 'spam'])\n",
    "\n",
    "# Calculate the number of non-spam (ham) messages\n",
    "# Subtract the number of spam messages from the total number of messages\n",
    "number_of_non_spam_msgs = total_msgs - number_of_spam_msgs\n",
    "\n",
    "# Calculate the prior probability of a message being spam\n",
    "# This is the ratio of spam messages to the total number of messages\n",
    "probability_of_spam_msgs = number_of_spam_msgs / total_msgs\n",
    "\n",
    "# Calculate the prior probability of a message being non-spam (ham)\n",
    "# This is the ratio of non-spam messages to the total number of messages\n",
    "probability_of_non_spam_msgs = number_of_non_spam_msgs / total_msgs\n",
    "\n",
    "# Print the results for verification\n",
    "print('Total number of messages:', total_msgs)  # The total count of all messages in the dataset\n",
    "print('Number of spam messages:', number_of_spam_msgs)  # The count of messages labeled as spam\n",
    "print('Number of non-spam messages:', number_of_non_spam_msgs)  # The count of messages labeled as non-spam\n",
    "print('Probability of spam messages:', probability_of_spam_msgs)  # Prior probability of spam messages\n",
    "print('Probability of non-spam messages:', probability_of_non_spam_msgs)  # Prior probability of non-spam messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Cleaning and Preprocessing Function**\n",
    "\n",
    "The `clean_text` function is used to clean and preprocess the text data in the dataset. This ensures that the text is in a standardized form for analysis. The function performs the following operations:\n",
    "\n",
    "1. **Lowercase Conversion**: Converts all text to lowercase to avoid any case sensitivity during analysis.\n",
    "2. **Remove Punctuation**: Removes all punctuation marks to standardize the words and ensure that only the text content is considered.\n",
    "3. **Tokenization or Splitting**: Splits the text into individual words, allowing for easier processing and analysis.\n",
    "\n",
    "The function returns a list of cleaned words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function to clean and preprocess the text data:\n",
    "# 1. Convert the text to lowercase (avoiding text sensitivity)\n",
    "# 2. Remove punctuation to standardize the words\n",
    "# 3. Split the text into words (tokenization)\n",
    "# 4. Return the cleaned text as a list of words\n",
    "def clean_text(text):\n",
    "    # Convert the text to lower case to avoid case sensitivity and remove punctuation\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Split the text into words (tokenization)\n",
    "    return text.split()\n",
    "\n",
    "# Apply the cleaning function to the 'Email_Messages' column and create a new column 'Cleaned_Email_Messages'\n",
    "sample_df['Cleaned_Email_Messages'] = sample_df['Email_Messages'].apply(clean_text)\n",
    "\n",
    "# Separate the Cleaned_Email_Messages into spam and non-spam messages based on the label\n",
    "spam_messages = sample_df[sample_df['Label'] == 'spam']\n",
    "non_spam_messages = sample_df[sample_df['Label'] == 'ham']\n",
    "\n",
    "# Count the number of words in spam and non-spam messages\n",
    "spam_words = Counter(word for message in spam_messages['Cleaned_Email_Messages'] for word in message)\n",
    "non_spam_words = Counter(word for message in non_spam_messages['Cleaned_Email_Messages'] for word in message)\n",
    "\n",
    "# Total number of spam and non-spam words\n",
    "total_spam_words = sum(spam_words.values())\n",
    "total_non_spam_words = sum(non_spam_words.values())\n",
    "\n",
    "# Set of unique words (union of words in both spam and non-spam messages)\n",
    "unique_words = set(spam_words).union(set(non_spam_words))\n",
    "\n",
    "# Total number of unique words in the entire vocabulary\n",
    "len_unique_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculate Likelihood Naive Bayes with Laplace Smoothing**\n",
    "\n",
    "This implementation calculates the likelihood of a word being spam or non-spam using **Naive Bayes with Laplace Smoothing**, which is preferred over **Maximum Likelihood Estimation (MLE)** for several reasons:\n",
    "\n",
    "1. **Handles Missing Words**: \n",
    "   - Laplace smoothing assigns a small probability to unseen words, unlike MLE, which assigns zero probability to any word not seen in the training data.\n",
    "\n",
    "2. **Zero Probability Prevention**: \n",
    "   - By adding a constant (usually 1), Laplace smoothing ensures that unseen words do not result in a zero probability, which could severely impact the model's performance.\n",
    "\n",
    "3. **Scalability**: \n",
    "   - Naive Bayes with Laplace smoothing is computationally efficient and works well even with large vocabularies and sparse text data (a common scenario in text classification).\n",
    "\n",
    "4. **Simplified Assumption**: \n",
    "   - The method assumes word independence given the class (spam or non-spam). This simplifying assumption often leads to good performance in practice, even though it may not hold true in all cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function calculates the likelihood of a word being spam or non-spam using Laplace smoothing\n",
    "# This is where we prefer Naive Bayes over MLE for several reasons:\n",
    "# 1.Naive Bayes accounts for missing words (words that might not appear in training data)\n",
    "# 2.It uses Laplace smoothing to handle zero probabilities, which MLE cannot handle unless manually adjusted\n",
    "# 3.Naive Bayes assumes word independence (which is a simplifying assumption but often works well in practice)\n",
    "def calculate_likelihood(word, spam_words, non_spam_words, total_spam_words, total_non_spam_words, len_unique_words, alpha=1):\n",
    "    # Get the count of the word in spam and non-spam messages\n",
    "    spam_word_count = spam_words.get(word, 0)\n",
    "    non_spam_word_count = non_spam_words.get(word, 0)\n",
    "    \n",
    "    # Laplace smoothing - adds alpha to the word count and total words to avoid zero probabilities\n",
    "    # This ensures we don't end up with a zero probability for unseen words\n",
    "    spam_likelihood = (spam_word_count + alpha) / (total_spam_words + len_unique_words)  # Spam likelihood\n",
    "    non_spam_likelihood = (non_spam_word_count + alpha) / (total_non_spam_words + len_unique_words)  # Non-spam likelihood\n",
    "    \n",
    "    # Without smoothing, using MLE directly could result in zero probabilities for unseen words\n",
    "    # Example of MLE without smoothing (commented out):\n",
    "    # spam_likelihood = spam_word_count / total_spam_words if total_spam_words > 0 else 0\n",
    "    # non_spam_likelihood = non_spam_word_count / total_non_spam_words if total_non_spam_words > 0 else 0\n",
    "\n",
    "    # Return both likelihoods for comparison (spam vs non-spam)\n",
    "    return spam_likelihood, non_spam_likelihood\n",
    "\n",
    "# Loop through the unique words in the vocabulary and calculate the likelihood of each word being spam and non-spam\n",
    "for word in unique_words:\n",
    "    # Get the likelihood of the word being spam and non-spam (with Laplace smoothing)\n",
    "    spam_likelihood, non_spam_likelihood = calculate_likelihood(word, spam_words, non_spam_words, total_spam_words, total_non_spam_words, len_unique_words)\n",
    "    \n",
    "    # Print out the likelihoods for each word\n",
    "    print('Word:', word)\n",
    "    print('Spam Likelihood (with Laplace Smoothing):', spam_likelihood)\n",
    "    print('Non Spam Likelihood (with Laplace Smoothing):', non_spam_likelihood)\n",
    "    \n",
    "    # Uncomment to compare with MLE approach (without smoothing)\n",
    "    # print('Spam Likelihood (MLE):', spam_word_count / total_spam_words if total_spam_words > 0 else 0)\n",
    "    # print('Non Spam Likelihood (MLE):', non_spam_word_count / total_non_spam_words if total_non_spam_words > 0 else 0)\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "# The Naive Bayes method is generally preferred over MLE for text classification tasks like spam detection:\n",
    "# 1.Laplace Smoothing: Naive Bayes uses Laplace smoothing (additive smoothing) to handle unseen words, which is a common issue in real-world data where some words may be missing in the training set.\n",
    "# 2.Independence Assumption**: Naive Bayes assumes that words are independent given the class (spam or non-spam). This assumption simplifies the computation and works surprisingly well in practice, even if the assumption is not perfectly true.\n",
    "# 3.Handling Zero Probabilities**: Without smoothing, MLE would assign a zero probability to unseen words, which can severely affect the model performance. Naive Bayes with smoothing avoids this issue, making it more robust in real-world applications.\n",
    "# 4.Scalability: Naive Bayes is computationally efficient, especially when working with large vocabularies and sparse data (as is common with text data).\n",
    "# 5.Interpretability**: Naive Bayes provides clear probabilities for each word being spam or non-spam, making it easy to interpret and understand the model's predictions.\n",
    "# 6.Loop through the unique words and calculate the likelihood of each word being spam and non-spam\n",
    "for word in unique_words:\n",
    "    # Get the likelihood of the word being spam and non-spam (with Laplace smoothing)\n",
    "    spam_likelihood, non_spam_likelihood = calculate_likelihood(word, spam_words, non_spam_words, total_spam_words, total_non_spam_words, len_unique_words)\n",
    "    print('Word:', word)\n",
    "    print('Spam Likelihood:', spam_likelihood)\n",
    "    print('Non Spam Likelihood:', non_spam_likelihood)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function: `calculate_posterior_probability`**\n",
    "\n",
    "This function calculates the posterior probabilities of a message being spam or non-spam using a Naive Bayes approach.\n",
    "\n",
    "#### **Parameters:**\n",
    "- `message`: The message to classify.\n",
    "- `spam_words`: A list of words that appear in spam messages.\n",
    "- `non_spam_words`: A list of words that appear in non-spam messages.\n",
    "- `total_spam_words`: The total count of words in spam messages.\n",
    "- `total_non_spam_words`: The total count of words in non-spam messages.\n",
    "- `len_unique_words`: The number of unique words in the dataset.\n",
    "- `probability_of_spam_msgs`: The prior probability of a message being spam.\n",
    "- `probability_of_non_spam_msgs`: The prior probability of a message being non-spam.\n",
    "- `alpha`: The Laplace smoothing parameter (default is 1).\n",
    "\n",
    "#### **Process:**\n",
    "1. **Clean the Message**: The message is cleaned by removing unnecessary characters or formatting.\n",
    "2. **Initialize Priors**: The prior probabilities for both spam and non-spam are set using `probability_of_spam_msgs` and `probability_of_non_spam_msgs`.\n",
    "3. **Loop Through Words**: For each word in the cleaned message:\n",
    "   - Calculate the likelihood of the word being in a spam or non-spam message using `calculate_likelihood`.\n",
    "   - Multiply the likelihoods for both classes to update the posterior probabilities.\n",
    "4. **Return Posterior Probabilities**: The function returns the posterior probabilities for both spam and non-spam classes.\n",
    "\n",
    "#### **Explanation:**\n",
    "The function computes the likelihood of the message being spam or non-spam by iterating over the words in the message, updating the priors with each word's likelihood. The posterior probability is then used to classify the message based on which class has a higher probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to calculate the posterior probability of the message being spam and non spam\n",
    "#It accepts the message, spam_words, non_spam_words, total_spam_words, total_non_spam_words, len_unique_words, probability_of_spam_msgs, probability_of_non_spam_msgs, and alpha as input\n",
    "def calculate_posterior_probability(message, spam_words, non_spam_words,\n",
    "                                               total_spam_words, total_non_spam_words,\n",
    "                                               len_unique_words, probability_of_spam_msgs,\n",
    "                                               probability_of_non_spam_msgs, alpha=1):\n",
    "    # Clean the message\n",
    "    cleaned_message = clean_text(message)\n",
    "    \n",
    "    # Initialize priors for both classes\n",
    "    spam_posterior = probability_of_spam_msgs\n",
    "    non_spam_posterior = probability_of_non_spam_msgs\n",
    "    \n",
    "    # Loop through each word in the cleaned message\n",
    "    for word in cleaned_message:\n",
    "        # Get the likelihood for the word in both spam and non-spam\n",
    "        spam_likelihood, non_spam_likelihood = calculate_likelihood(\n",
    "            word, spam_words, non_spam_words, total_spam_words, total_non_spam_words,\n",
    "            len_unique_words, alpha\n",
    "        )\n",
    "        \n",
    "        # Multiply the likelihoods we donot need to divide by the evidence because we are comparing the posterior probabilities\n",
    "        spam_posterior *= spam_likelihood\n",
    "        non_spam_posterior *= non_spam_likelihood\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Return the posterior probabilities for both classes\n",
    "    return spam_posterior, non_spam_posterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classifying Messages as Spam or Non-Spam**\n",
    "\n",
    "In this section, we classify a list of sample email messages as either spam or non-spam based on posterior probabilities calculated using a Naive Bayes classifier.\n",
    "\n",
    "#### **Steps:**\n",
    "1. **Messages to Classify**: A list of sample messages is provided.\n",
    "2. **Posterior Probabilities Calculation**: For each message, we compute the posterior probabilities for spam and non-spam using the `calculate_posterior_probability` function.\n",
    "3. **Classification**: The message is classified as spam if the posterior probability for spam is higher than that for non-spam. Otherwise, it is classified as non-spam.\n",
    "4. **Output**: For each message, the spam and non-spam posterior probabilities are printed, followed by the classification result.\n",
    "\n",
    "#### **Example Output**:\n",
    "- For each message, the spam and non-spam posterior probabilities are shown along with the classification result. The algorithm outputs whether the message is classified as \"Spam\" or \"Non-Spam\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    'WINNER This is the secret code to unlock the money: C3421.',\n",
    "    'Sounds good, Tom, then see u there',\n",
    "    '',\n",
    "    \"You won't believe it but it's true. It's Incredible Txts! Reply G now to learn truly amazing things that will blow your mind. From O2FWD only 18p/txt\",\n",
    "    'You have a limited time offer to claim your free vacation!',\n",
    "    'Congratulations! You have been selected for a special prize.',\n",
    "    'Reminder: Your account has been suspended. Please update your information.',\n",
    "    'Free gift card for you! Respond now to claim your prize.',\n",
    "    'Hey, are we still on for dinner tonight? Let me know!',\n",
    "    'Important: Verify your account to avoid service interruptions.',\n",
    "    'Limited-time offer: Buy one, get one free on all products!',\n",
    "    'Alert: Your password was recently changed. If this wasnâ€™t you, click here.',\n",
    "    'Get an exclusive deal now! Click here to claim your discount.'\n",
    "]\n",
    "\n",
    "\n",
    "# Loop through the messages and classify each message as spam or non-spam\n",
    "for message in messages:\n",
    "    # Calculate the posterior probabilities\n",
    "    spam_posterior, non_spam_posterior = calculate_posterior_probability(\n",
    "        message, spam_words, non_spam_words, total_spam_words, total_non_spam_words, \n",
    "        len_unique_words, probability_of_spam_msgs, probability_of_non_spam_msgs\n",
    "    )\n",
    "    # Print the results\n",
    "    print('Message:', message)\n",
    "    print('Spam Posterior:', spam_posterior)\n",
    "    print('Non Spam Posterior:', non_spam_posterior)\n",
    "\n",
    "    # Classify the message\n",
    "    if spam_posterior > non_spam_posterior:\n",
    "        print('Message is Spam')\n",
    "    else:\n",
    "        print('Message is Non Spam')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating the Spam Detection Algorithm**\n",
    "\n",
    "This block of code evaluates the performance of the spam detection algorithm on the entire dataset. It calculates the posterior probabilities for each email, comparing the probabilities for spam and non-spam classes to classify the message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialize variables for predictions and actual labels\n",
    "predictions = []  # This will hold the predicted labels (spam/ham)\n",
    "actual_labels = df['Label']  # This holds the actual labels from the dataset (spam/ham)\n",
    "\n",
    "# Loop through each message and its corresponding label in the dataset\n",
    "for message, label in zip(df['Email_Messages'], df['Label']):\n",
    "    # Calculate the posterior probabilities for both spam and non-spam using the trained model\n",
    "    # 'spam_words', 'non_spam_words', 'total_spam_words', 'total_non_spam_words', 'len_unique_words' \n",
    "    # are all parameters learned from the training data.\n",
    "    spam_posterior, non_spam_posterior = calculate_posterior_probability(\n",
    "        message, spam_words, non_spam_words, total_spam_words, total_non_spam_words, \n",
    "        len_unique_words, probability_of_spam_msgs, probability_of_non_spam_msgs\n",
    "    )\n",
    "    \n",
    "    # Classify the message as spam if the posterior probability for spam is higher\n",
    "    if spam_posterior > non_spam_posterior:\n",
    "        predictions.append('spam')  # If the message is more likely to be spam, append 'spam'\n",
    "    else:\n",
    "        predictions.append('ham')  # Otherwise, append 'ham' (non-spam)\n",
    "\n",
    "# Calculate the confusion matrix to evaluate the performance of the classifier\n",
    "# The confusion matrix will compare the actual labels vs predicted labels\n",
    "conf_matrix = confusion_matrix(actual_labels, predictions, labels=['spam', 'ham'])\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score for the spam class\n",
    "# These metrics help evaluate the classifier's performance for identifying spam messages\n",
    "precision = precision_score(actual_labels, predictions, pos_label='spam') \n",
    "recall = recall_score(actual_labels, predictions, pos_label='spam')  \n",
    "f1 = f1_score(actual_labels, predictions, pos_label='spam')  \n",
    "\n",
    "# Print evaluation metrics to assess model performance\n",
    "print(\"Total messages:\", len(df))  # Print the total number of messages in the dataset\n",
    "print(\"Accuracy:\", sum(1 for p, l in zip(predictions, actual_labels) if p == l) / len(df))  # Accuracy: Percentage of correctly classified messages\n",
    "print(\"Precision:\", precision)  # Precision: Proportion of true positive spam out of predicted spam\n",
    "print(\"Recall:\", recall)  # Recall: Proportion of true positive spam out of all actual spam\n",
    "print(\"F1-Score:\", f1)  # F1-Score: Harmonic mean of precision and recall\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
